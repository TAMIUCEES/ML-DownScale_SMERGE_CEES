{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0H5MbMwZU8bJ"
      },
      "source": [
        "# Introduction\n",
        "\n",
        "The Machine Learning technique used will be eXtreme Gradient Boost (XGBoost), specifically XGBoost Regressor with verifier. XGBoost regressor is a machine learning algorithm that excels in predictive accuracy. XGBoost regression model builds a series of trees one after another, where each new tree learns from the previous created tree's errors and improves until it reaches its maximum improvement. XGBoost, in contrast to Gradient Boosting, introduces several optimizations such as improved regularization, can handle larger data, is more adaptable to various problems, and has built-in capabilities to handle missing data.\n",
        "\n",
        "---\n",
        "\n",
        "Before running the code, first check if SHAP is already in the library. If it is not, install using given code, the same applies to *os*. SHAP is an interpretability tool that will help us to see the contribution of each feature towards machine learning predictions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Us_1swemU2-g"
      },
      "outputs": [],
      "source": [
        "pip install shap"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e1OOKOaraxGN"
      },
      "outputs": [],
      "source": [
        "pip install os"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0wIa6usNayyg"
      },
      "source": [
        "The following libraries will be imported to be able to run the code.\n",
        "\n",
        "\n",
        "First, we have the most important library that is XGBoost, which will help to import our base model *XGBRegressor*.\n",
        "\n",
        " See the link below for more information on the package:\n",
        "\n",
        "> -  XGBRegressor - https://xgboost.readthedocs.io/en/stable/python/python_api.html#xgboost.XGBRegressor\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "Since we are working with XGBoost, most of the libraries used are from \"sklearn package\", which includes:\n",
        "\n",
        "**sklearn-metrics**\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "- **r2_score** - Used to measure how well a linear regression model fits the data.\n",
        "> For more information visit: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.r2_score.html\n",
        "\n",
        "- **mean_squared_error** - Evaluates the closeness between the regression line and the set of data points. A lower MSE indicates that the regression line is a better fit for the data.\n",
        "> For more information visit: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_squared_error.html\n",
        "\n",
        "---\n",
        "\n",
        "Additionally, we have SHAP and os, as explained before, SHAP is an interpretabilty tool which helps to evaluate the contribution of each feature towards predictions. Os is particularly used to concatenate path components with exactly one directory separator \"/\", and create one path.\n",
        "  \n",
        "> See the links below for more information in the packages:\n",
        "- **SHAP** - https://shap.readthedocs.io/en/latest/\n",
        "- **os** - https://www.geeksforgeeks.org/python-os-path-join-method/\n",
        "\n",
        "The other packages contain python features such as 'pandas' and 'numpy', which helps obtain the mean and standard deviation.\n",
        "\n",
        "> See the links below to more information in the packages:\n",
        "- **Pandas** - https://pandas.pydata.org/docs/user_guide/index.html\n",
        "- **NumPy** - https://numpy.org/doc/stable/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y9w2SgKja864"
      },
      "outputs": [],
      "source": [
        "from xgboost import XGBRegressor\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import shap\n",
        "from sklearn.metrics import r2_score\n",
        "from sklearn.metrics import mean_squared_error as mse"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QERvjeMWb-qq"
      },
      "source": [
        "- The user will then be asked to input the name of the dataset.\n",
        "\n",
        "  An example would be:\n",
        "  > *Enter the name of the dataset:* **Demo**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "69ytP7Jfb_ch"
      },
      "outputs": [],
      "source": [
        "name = input(\"Enter the name of the dataset: \")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nFhHt5zAb_m5"
      },
      "source": [
        "- Paths will be constructed based on the zone and name.\n",
        "\n",
        "  An example would be:\n",
        "  > *Enter the home directory where the datasets are located:* **/mnt/e/ML-DownScale_SMERGE_CEES-main**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Afuz1-42b_wl"
      },
      "outputs": [],
      "source": [
        "home_directory = input(\"Enter the home directory where the datasets are located: \")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vVhIF5kqciQK"
      },
      "source": [
        "- Input the name of the testing dataset, this will be used to construct the full path.\n",
        "\n",
        "  An example would be:\n",
        "> *Enter the name of the testing dataset:* **Demo_1000_test.csv**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "42dKd6nNcif8"
      },
      "source": [
        "- While the code is running it will also be checking if the testing file exists. If the file does not exist, then the code will prompt the user until a valid file is provided."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "24dLIIJ2ciYo"
      },
      "outputs": [],
      "source": [
        "test_dataset_name = input(\"Enter the name of the testing dataset (including file extensions): \")\n",
        "\n",
        "test_file = os.path.join(home_directory, test_dataset_name)\n",
        "\n",
        "while not os.path.exists(test_file):\n",
        "  print(\"The specified training file does not exist. Enter the correct dataset name. \")\n",
        "  test_dataset_name = input(\"Enter the name of the testing dataset (including file extensions): \")\n",
        "  test_file = os.path.join(home_directory, test_dataset_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q8IBz4Hjciwl"
      },
      "source": [
        "- Input the name of the training dataset, this will be used to construct the full path.\n",
        "\n",
        "  An example would be:\n",
        "  > *Enter the name of the training dataset:* **Demo_1000_train.csv**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oNH0MQFVe2wn"
      },
      "source": [
        "- While the code is running it will also be checking if the training file exists. If the file does not exist, then the code will prompt the user until a valid file is provided."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UjWiFBAxey27"
      },
      "outputs": [],
      "source": [
        "train_dataset_name = input(\"Enter the name of the training dataset (including file extensions): \")\n",
        "\n",
        "train_file = os.path.join(home_directory, train_dataset_name)\n",
        "\n",
        "while not os.path.exists(train_file):\n",
        "  print(\"The specified training file does not exist. Enter the correct dataset name.\")\n",
        "  train_dataset_name = input(\"Enter the name of the training dataset (including file extensions): \")\n",
        "  train_file = os.path.join(home_directory, train_dataset_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fR1KGEmF1_Vg"
      },
      "source": [
        "# Variables\n",
        "- Prior to exporting testing and training data, it is necessary to create and upload a text file list (comma-separated values) containing the names of all variables (including the target variable). The text file must be saved in the same folder as the training and testing files.\n",
        "\n",
        "\n",
        "- Given that the variable names can vary depending on the user's data, it is more convenient to create a separate file with the variable names and then adjust them to the code.\n",
        "\n",
        "- An example of the list of variables would be:\n",
        "\n",
        "  > *Enter the name of the variables list (make sure the file is in text file format):* **vars.txt**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DRc9U_-G2CZK"
      },
      "outputs": [],
      "source": [
        "variables_list_name = input(\"Enter the name of the variables list (make sure the file is in text file format): \")\n",
        "variables_file = os.path.join(home_directory, variables_list_name)\n",
        "\n",
        "while not os.path.exists(variables_file):\n",
        "  variables_list_name = input(\"The variable list txt was not found, it should be in the home directory, try again. \")\n",
        "  variables_file = os.path.join(home_directory, variables_list_name)\n",
        "\n",
        "text_file = open(variables_file, \"r\")\n",
        "variables = text_file.read().split(',')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "plhDHAyXfb70"
      },
      "source": [
        "# Testing Data & Verifier\n",
        "\n",
        "- First, we created a variable called *test_data* to read our testing data, then using *variables* from the previous section we will save all of the data into *test_data*. Testing data is going to help us measure the performance of our model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YGcgu2D1pMea"
      },
      "outputs": [],
      "source": [
        "test_data = pd.read_csv(test_file)\n",
        "test_data = test_data.loc[:, variables]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JND6cY8Q22Qa"
      },
      "source": [
        ">**Warning:**\n",
        "Before separating the predictor variables from the target variable, we created a filter to make sure the data saved will be compatible with XGBoost. This machine learning module is created to deal with numeric values only.\n",
        "\n",
        "- The following code will remove all the *object type* column variables from the testing set, create new variables that will hold those columns, and eventually create a list of all of the columns that will be removed.\n",
        "  - **Object type** columns variables refers to the columns that contain a combination of strings, letters, and numbers. XGBoost is compatible only with numeric values.\n",
        "\n",
        "- As a result, prior to applying the testing set to the XGBoost model, we must eliminate those columns.\n",
        "\n",
        "- However, if you want to keep that variable as part of your variables for the model, make sure to format that column beforehand for both testing and training sets.\n",
        "\n",
        "- In our case, we dealt with a variable called  *PageName*.\n",
        "  - **PageName** - This variable holds string values and can not be used in our model. However, the variable holds the position grids of the data by coordinates with letters and numbers such as B2.\n",
        "\n",
        "- Although **PageName** was not part of the XGBoost model predictions, it was saved in a variable separately so that it could be added back into the final prediction output. The variable will be called *index variable*. If this is the case for you, and you want to save your data to place back into the output, you will be able to use the variables that this section will create. Otherwise, if you are just planning to remove them, you can just run this section and move on to the next one."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yf1zJqaIYhAX"
      },
      "outputs": [],
      "source": [
        "def process_object_columns(dataframe, dex):\n",
        "  def get_object_columns(df):\n",
        "    return list(df.select_dtypes(include=['object']).columns)\n",
        "\n",
        "  # Function to get object type columns\n",
        "  object_columns = get_object_columns(dataframe)\n",
        "\n",
        "  if dex in object_columns:\n",
        "    object_columns.remove(dex)\n",
        "\n",
        "  print(\"Object type columns:\", object_columns)\n",
        "  # Removing columns with 'object' dtype\n",
        "  df_without_objects = dataframe.drop(object_columns, axis=1)\n",
        "\n",
        "  # Creating separate variables for object type columns with their respective names\n",
        "  for col in object_columns:\n",
        "    globals()[f\"{col}\"] = dataframe[col]\n",
        "\n",
        "  # Printing separate variables\n",
        "  for col in object_columns:\n",
        "    print(f\"{col}:\")\n",
        "    print(globals()[f\"{col}\"])\n",
        "\n",
        "  # Returning the original DataFrame without object type columns\n",
        "  return df_without_objects"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Input the name of the index variable, which will be integrated in the output dataframe.\n",
        "> *Enter the index variable from the list of variables:* **PageName**\n"
      ],
      "metadata": {
        "id": "f4dgkiEmNyLu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "index_variable = input(\"Enter the index variable from the list of variables: \")"
      ],
      "metadata": {
        "id": "NvSPcuTqzHmR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5PKYT5J63HdR"
      },
      "outputs": [],
      "source": [
        "test_data = process_object_columns(test_data, index_variable)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "06YKVuG73hQI"
      },
      "source": [
        "*   After removing the object type columns, this is how the new test data looks like:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o9z1PKYXKtCa"
      },
      "outputs": [],
      "source": [
        "test_page = test_data[index_variable]\n",
        "test_data.drop(columns=[index_variable], inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yFEuhrJR3Tpy"
      },
      "outputs": [],
      "source": [
        "print(test_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "369YIndi3S3a"
      },
      "source": [
        "- The next step is to create a variable that holds date and verifier columns:\n",
        "\n",
        "  - Since the *date* column is a component of our predictor variables, we must format its numerical values to a value that XGBoost can interpret as a date, rather than just a number. However, before formatting the *date* column, we create this **date** variable to preserve its original format and then insert it back into the output frame. The date format that can be taken can be found in the **Date Formatting** section.\n",
        "\n",
        "  - The **ver** variable was created to hold the in situ data (Verifier), which can also be referred to as validation data. We created this variable to hold the validation data and place it in the output file so that we can compare the results in the final output. However, since **ver** is not used as a predictor, it will be removed from **test_data**.\n",
        "\n",
        "   > **Note:** If you have two verifiers, you must include the second one in your variables text file and create another line of code to drop that verifier column.\n",
        "Use the following code to drop that second verifier. Copy the code, change the *Verifier* to the name of your second verifier and paste it in the next section of code.\n",
        "```\n",
        "test_data = test_data.drop(\"Verifier\", axis=1)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8BYIfKDH2XeF"
      },
      "outputs": [],
      "source": [
        "date = pd.read_csv(test_file, usecols=['Date'])\n",
        "ver = pd.read_csv(test_file, usecols=['Verifier'])\n",
        "\n",
        "#Removing verifier column\n",
        "test_data = test_data.drop(\"Verifier\", axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Nt7iKTh5ryv"
      },
      "source": [
        "# Date Formatting\n",
        "- For regression models it is important to follow the required formatting to make sure to have a high performance. This section will change the date format to a *to_datetime* format which works better with machine learning regression models. Therefore, there will be a date formatting for both testing and training sets.\n",
        ">**Note:**\n",
        "> - This section only allows three types of date formats: \"%m/%d/%Y\", \"%Y/%m/%d\", and \"%Y/%j\". If you are using another type of format make sure to change it to one of those formats beforehand."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j-dnW4ChdUak"
      },
      "outputs": [],
      "source": [
        "def date_formatting(dataframe2):\n",
        "    # Processing 'Date' column for testing\n",
        "    date_format = input('If file use \"%m/%d/%Y\" type: A, if \"%Y/%m/%d\" type: B, else \"%Y/%j\" type C: ')\n",
        "    #date_column = dataframe2[column_name]\n",
        "    while date_format != 'A' and date_format != 'B' and date_format != 'C':\n",
        "\n",
        "        date_format = input('Input ERROR. If file use \"%m/%d/%Y\" type: A, if \"%Y/%m/%d\" type: B, else if \"%Y/%j\" type C: ')\n",
        "\n",
        "    if date_format == 'A':\n",
        "\n",
        "        #date_format = '%m%d%Y'\n",
        "        dataframe2['Date'] = pd.to_datetime(date['Date'], format=\"%m/%d/%Y\")\n",
        "        dataframe2['Date'] =  dataframe2['Date'].astype(int)\n",
        "\n",
        "    if date_format == 'B':\n",
        "\n",
        "        #date_format = '%Y/%m/%d'\n",
        "        dataframe2['Date'] = pd.to_datetime(date['Date'], format = '%Y/%m/%d')\n",
        "        #.astype(int)\n",
        "        dataframe2['Date'] =  dataframe2['Date'].astype(int)\n",
        "\n",
        "    if date_format == 'C':\n",
        "\n",
        "        #date_format = '%Y/%j'\n",
        "        dataframe2['Date'] = pd.to_datetime(date['Date'], format=\"%Y/%j\")\n",
        "        dataframe2['Date'] =  dataframe2['Date'].astype(int)\n",
        "    #else:\n",
        "      #print (\"Data format not recognized, make sure your data is formatted as %m%d%Y, %Y%m%d, or %Y%j\")\n",
        "     # date_format = 0\n",
        "    return dataframe2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LZB7kqrW5xHP"
      },
      "outputs": [],
      "source": [
        "test_data = date_formatting(test_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lVRBIzXC5ydO"
      },
      "source": [
        "*   Lastly, to get the testing data ready, you need to separate the predictor variables from the target variable.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "enljH6f756Pt"
      },
      "outputs": [],
      "source": [
        "# Separating target variable (dependent) from the initial variables (independent).\n",
        "target_variable = input(\"Write the name of the target variable to extract it from the list of variables: \")\n",
        "variables = test_data.columns.tolist()\n",
        "\n",
        "while target_variable in variables:\n",
        "    variables.remove(target_variable)\n",
        "\n",
        "y_test = test_data[target_variable]\n",
        "x_test = test_data[variables]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G8QyMZLC23WN"
      },
      "source": [
        "# Training Data\n",
        "\n",
        "- First, we create a variable called *train_data* to read our training data. Then, using the *variables* from the previous section, we save all of the data into *train_data*. The training data will be used to create our model and make it able to create predictions.\n",
        "\n",
        "  > **Note:**\n",
        "  > - Since we already removed the object type columns in a previous section we will use  *variables* from the **Variables** section to retrieve just the numerical columns values and define **x_train**. Additionally, we will use **target_variable** from testing data to also define **y_train** in the training data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RARn8_5m6BP2"
      },
      "outputs": [],
      "source": [
        "train_data = pd.read_csv(train_file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0vjJtn2b6Dr-"
      },
      "source": [
        "- Prior to defining the predictor variables (**x_train**) and target variable (**y_train**) for training, we must also format the date for the training data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fAK4Hr7YdqXH"
      },
      "outputs": [],
      "source": [
        "train_data = date_formatting(train_data)\n",
        "try:\n",
        "    train_data.drop(columns=[index_variable], inplace=True)\n",
        "except:\n",
        "    print()\n",
        "try:\n",
        "    variables.remove(target_variable)\n",
        "    variables.remove(index_variable)\n",
        "except:\n",
        "    print()\n",
        "# Defining x_train and y_train\n",
        "train_data = train_data[variables +[target_variable]]\n",
        "train_data.dropna(inplace=True)\n",
        "x_train = train_data[variables]\n",
        "y_train = train_data[target_variable]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fA3blbaX5P0R"
      },
      "source": [
        "# eXtreme Gradient Boost parameters\n",
        "\n",
        "eXtreme Gradient Boost is a ML-technique with its own package of tools and functions. XGBoost is a tree-based algorithm that builds a series of decision trees to make accurate predictions, while controlling the model complexity with regularization. However, as other machine learning models, to improve the machine learning model performance, we need to tune the parameters of the model and get the best parameters also known as hypertunning parameters. The hypertuning parameters were conducted using an iteration method. A series of tuning values were tested and the best parameters were found after hundreds of trials.\n",
        "\n",
        "---\n",
        "\n",
        "- **verbose** - *int*; Enable verbose output:\n",
        "  - If 1 is specified, the program will print progress and performance information  periodically (less frequently for larger trees).\n",
        "  - If a value greater than 1 is specified, the program will print progress and performance information for every tree.\n",
        "- **n_estimators** - *int*; The number of times to repeat the process of training a model on the residuals of the previous model. A large number of iterations will result in better performance.\n",
        "- **max_depth** - *int* or *None*; The maximum number of levels in the decision tree.\n",
        "- **tree_method** - *string*; The algorithm method to use for building decision trees, which can depend on training speed and memory usage.\n",
        "\n",
        "> For more information visit: https://xgboost.readthedocs.io/en/stable/parameter.html\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "Before jumping into the machine learning configuration, there is one last processing step we need to take. Since machine learning models work best with numbers, and the formatting of the data gathered can sometimes vary, we need to convert our *x_train*, *y_train*, and *x_test* into number types using the numpy function to ensure that our algorithm runs smoothly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k_NxTr1oXFX4"
      },
      "outputs": [],
      "source": [
        "x_train_nu = x_train.to_numpy()\n",
        "y_train_nu = y_train.to_numpy()\n",
        "x_test_nu = x_test.to_numpy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ok_LDhhwXFka"
      },
      "source": [
        "*Configuring the eXtreme Gradient Boost Regressor*\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "- The parameters that were used for XGBoost are inside the XGBRegressor function.\n",
        "\n",
        "\n",
        "- The variable *'mse'* represents the mean squared error and is obtained by using the target variable (dependent variable) and the regression prediction of the initial variables (independent variables)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ViGs01l0i0qW"
      },
      "outputs": [],
      "source": [
        "model = XGBRegressor(verbosity=1, n_estimators=500, max_depth=10, tree_method='gpu_hist')\n",
        "model.fit(x_train_nu, y_train_nu)\n",
        "\n",
        "MSE = mse(y_train_nu, model.predict(x_train_nu))\n",
        "print(\"The mean squared error (MSE) on the set: {:.4f}\".format(MSE))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dT8NBqpV2s-3"
      },
      "source": [
        "- By obtaining the prediction of the test model, we will prepare for the output dataframe.\n",
        "\n",
        "- Then we took the date that we saved earlier with its original format and incorporated it back to the output dataframe. The same steps were made for the index variable, ML, and Verifier.\n",
        "\n",
        "- Finally, we saved the output data frame as *.csv* using the name you inputted at the beginning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lfS3JqZikrNO"
      },
      "outputs": [],
      "source": [
        "predictions = model.predict(x_test_nu)\n",
        "test_data['Date'] = date\n",
        "test_data['ML'] = predictions\n",
        "test_data['Verifier'] = ver\n",
        "test_data[index_variable] = test_page\n",
        "\n",
        "## Saving the output dataframe to a CSV file\n",
        "test_data.to_csv(name + \".csv\", index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CarhCteF9_T5"
      },
      "source": [
        "# SHapley Additive exPlanations (SHAP)\n",
        "\n",
        "SHAP states the output of the ML model by carrying out the contribution from each variable to the prediction. SHAP was used to evaluate independent variable sensitivity. SHAP uses a game theory-based methodology that calculates feature importance by comparing a model's predictions with and without a particular feature. For our case, we used a random sample of 1000 from the train data. We then apply SHAP to analyze those 1000 row samples and calculate the contribution of each feature for the 1000 predictions. SHAP assigns values according to the mean marginal contribution of each feature across all possible values. Using this method, you will be able to see a bar graph that illustrates the individual contribution of each feature. The graph is arranged from highest contribution to lowest contribution feature.\n",
        "\n",
        "> For more information visit: https://shap.readthedocs.io/en/latest/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1kGbapw1n71z"
      },
      "outputs": [],
      "source": [
        "# Generating variable importance plots\n",
        "X_sampled = x_train.sample(1000, random_state=10, replace=True)\n",
        "\n",
        "explainer = shap.Explainer(model, X_sampled)\n",
        "shap_values = explainer(X_sampled)\n",
        "number_variables = len(variables)\n",
        "shap.plots.bar(shap_values, max_display=number_variables)\n",
        "\n",
        "## Saving variable importance plot\n",
        "mean_shap = np.abs(shap_values.values).mean(axis=0)\n",
        "shap_pd = pd.DataFrame(mean_shap, index=X_sampled.columns).sort_values(by=[0], ascending=False)\n",
        "shap_pd.to_csv(name + '_SHAP.csv')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}