{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KkD0XrwZZ0k5"
      },
      "source": [
        "# Introduction\n",
        "\n",
        "The Machine Learning technique used will be Gradient Boosting, specifically Gradient Boosting without a verifier. Gradient Boosting Regressor is a machine learning algorithm that uses a series of decision trees to make predictions. Each tree is trained to correct the errors of the previous trees, which results in a more accurate overall prediction.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "Before running the code, first check if SHAP is already in library. If it is not, install using given code, the same applies to *os*. SHAP is an interpretability tool that will help us to see the contribution of each feature towards machine learning predictions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UulX3R6GZytk"
      },
      "outputs": [],
      "source": [
        "pip install shap"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mOYXaOuACw6y"
      },
      "outputs": [],
      "source": [
        "pip install os"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XYCA2ilZC0Fe"
      },
      "source": [
        "The following libraries will be imported to be able to run the code.\n",
        "\n",
        "Since we are working with Gradient Boost, most of the libraries used are from \"sklearn package\", which includes:\n",
        "\n",
        "**sklearn-metrics**\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "- **accuracy_score** - The function calculates the accuracy of a subset.\n",
        "> For more information visit: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html\n",
        "\n",
        "- **r2_score** - Used to measure how well a linear regression model fits the data.\n",
        "> For more information visit: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.r2_score.html\n",
        "\n",
        "- **mean_squared_error** - Evaluates the closeness between the regression line and the set of data points. A lower MSE indicates that the regression line is a better fit for the data.\n",
        "> For more information visit: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_squared_error.html\n",
        "\n",
        "\n",
        "**sklearn**\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "- **dataset** - Embeds some datasets, they can be small refers as toy datasets or large as real-world datasets so the user can apply it to its machine learning algorithm.\n",
        "> For more information visit: https://scikit-learn.org/stable/datasets.html\n",
        "- **ensemble** - Allows for the optimization of arbitrary differentiable loss function of the machine learning model.\n",
        "> For more information visit: https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html#sklearn.ensemble.GradientBoostingRegressor\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "Additionally we have SHAP and os, as explained before, SHAP is an interpretabilty tool which helps to evaluate the contribution of each feature towards predictions. Os is particularly used to concatenate path components with exactly one directory separator \"/\", and create one path.\n",
        "  \n",
        " See the links below for more information in the packages:\n",
        "> - **SHAP** - https://shap.readthedocs.io/en/latest/\n",
        "> - **os** - https://www.geeksforgeeks.org/python-os-path-join-method/\n",
        "\n",
        "The other packages contain python features such as 'pandas' and 'numpy', which helps obtain the mean and standard deviation.\n",
        "\n",
        " See the links below to more information in the packages:\n",
        "> - **Pandas** - https://pandas.pydata.org/docs/user_guide/index.html\n",
        "> - **Numpy** - https://numpy.org/doc/stable/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cPWjhXpXEh5m"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import shap\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from numpy import mean, std\n",
        "from sklearn import datasets, ensemble\n",
        "from sklearn.metrics import r2_score\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.metrics import mean_squared_error as mse"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NLdH4kk6FHdI"
      },
      "source": [
        "- The user will then be asked to input the name of the dataset.\n",
        "\n",
        " An example would be:\n",
        " > *Enter the name of the dataset:* **GBR_BigRunWS**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "64po8ZOmFdUP"
      },
      "outputs": [],
      "source": [
        "name = input(\"Enter the name of the dataset: \")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q70NZytbFljg"
      },
      "source": [
        "- Paths will be constructed based on the zone and name.\n",
        "\n",
        " An example would be:\n",
        "> *Enter the home directory where the datasets are located:* **/mnt/e/share/BIgRun/ML-DownScale_SMERGE_CEES-main**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WJEPUSiOG_z7"
      },
      "outputs": [],
      "source": [
        "home_directory = input(\"Enter the home directory where the datasets are located: \")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LeNsrAQSH_4g"
      },
      "source": [
        "- Input the name of the testing dataset, this will be used to construct the full path.\n",
        "\n",
        " An example would be:\n",
        " > *Enter the name of the testing dataset:* **BigRunWS_500_inst.csv**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FqZy31TUIyG-"
      },
      "source": [
        "- While the code is running it will also be checking if the testing file exists. If the file does not exist, then the code will prompt the user until a valid file is provided."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kS7Ya5FNItme"
      },
      "outputs": [],
      "source": [
        "test_dataset_name = input(\"Enter the name of the testing dataset (including file extensions): \")\n",
        "\n",
        "test_file = os.path.join(home_directory, test_dataset_name)\n",
        "\n",
        "while not os.path.exists(test_file):\n",
        "  print(\"The specified training file does not exist. Enter the correct dataset name.\")\n",
        "  test_dataset_name = input(\"Enter the name of the testing dataset (including file extensions): \")\n",
        "  test_file = os.path.join(home_directory, test_dataset_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "obvT9pM4LMwO"
      },
      "source": [
        "- Input the name of the training dataset, this will be used to construct the full path.\n",
        "\n",
        " An example would be:\n",
        " > *Enter the name of the training dataset:* **BigRunWS_500_train_4.csv**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gu41Ut-VNoOj"
      },
      "source": [
        "- While the code is running it will also be checking if the training file exists. If the file does not exist, then the code will prompt the user until a valid file is provided."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y4QVu0KRMz23"
      },
      "outputs": [],
      "source": [
        "train_dataset_name = input(\"Enter the name of the training dataset (including file extensions): \")\n",
        "\n",
        "train_file = os.path.join(home_directory, train_dataset_name)\n",
        "\n",
        "while not os.path.exists(train_file):\n",
        "  print(\"The specified training file does not exist. Enter the correct dataset name.\")\n",
        "  train_dataset_name = input(\"Enter the name of the training dataset (including file extensions): \")\n",
        "  train_file = os.path.join(home_directory, train_dataset_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FMYGKPgBaRva"
      },
      "source": [
        "# Variables\n",
        "\n",
        "- Prior to exporting testing and training data, it is necessary to create and upload a text file list (comma-separated values) containing the names of all variables (including the target variable). The text file must be saved in the same folder as the training and testing files.\n",
        "\n",
        "- Given that the variable names can vary depending on the user's data, it is more convenient to create a separate file with the variable names and then adjust them to the code.\n",
        "\n",
        "- An example of the list of variables would be:\n",
        "\n",
        " > *Enter the name of the variables list (make sure the file is in text file format):* **vars.txt**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oQ3LECY8aVQT"
      },
      "outputs": [],
      "source": [
        "variables_list_name = input(\"Enter the name of the variables list (make sure the file is in text file format): \")\n",
        "variables_file = os.path.join(home_directory, variables_list_name)\n",
        "\n",
        "while not os.path.exists(variables_file):\n",
        "  variables_list_name = input(\"The variable list txt was not found, it should be in the home directory, try again \")\n",
        "  variables_file = os.path.join(home_directory, variables_list_name)\n",
        "\n",
        "text_file = open(variables_file, \"r\")\n",
        "variables = text_file.read().split(',')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a8oOa7_5PVgx"
      },
      "source": [
        "# Testing Data\n",
        "\n",
        "- First, we created a variable called *test_data* to read our testing data, then using *variables* from the previous section we will save all of the data into *test_data*. Testing data is going to help us measure the performance of our model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S1YzghQEQnES"
      },
      "outputs": [],
      "source": [
        "test_data = pd.read_csv(test_file)\n",
        "test_data = test_data.loc[:, variables]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "piY4Gy8RahR_"
      },
      "source": [
        ">**Warning:**\n",
        "Before separating the predictor variables from the target variable, we created a filter to make sure the data saved will be compatible with Gradient Boost. This machine learning module is created to deal with numeric values only.\n",
        "\n",
        "- The following code will remove all the *object type* column variables from the testing set, create new variables that will hold those columns, and eventually create a list of all of the columns that will be removed.\n",
        "  - **Object type** columns variables refers to the columns that contain a combination of strings, letters, and numbers. Gradient Boost is compatible only with numeric values.\n",
        "\n",
        "- As a result, prior to applying the testing set to the Gradient Boost model, we must eliminate those columns.\n",
        "\n",
        "- However, if you want to keep that variable as part of your variables for the model, make sure to format that column beforehand for both testing and training sets.\n",
        "\n",
        "- In our case, we dealt with a variable called  *PageName*.\n",
        "  - **PageName** - This variable holds string values and can not be used in our model. However, the variable holds the position grids of the data by coordinates with letters and numbers such as B2.\n",
        "\n",
        "- Although **PageName** was not part of the Gradient Boost model predictions, it was saved in a variable separately so that it could be added back into the final prediction output. The variable will be called *index variable*. If this is the case for you, and you want to save your data to place back into the output, you will be able to use the variables that this section will create. Otherwise, if you are just planning to remove them, you can just run this section and move on to the next one."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yf1zJqaIYhAX"
      },
      "outputs": [],
      "source": [
        "def process_object_columns(dataframe, dex):\n",
        "  def get_object_columns(df):\n",
        "    return list(df.select_dtypes(include=['object']).columns)\n",
        "\n",
        "  # Function to get object type columns\n",
        "  object_columns = get_object_columns(dataframe)\n",
        "\n",
        "  if dex in object_columns:\n",
        "    object_columns.remove(dex)\n",
        "\n",
        "  print(\"Object type columns:\", object_columns)\n",
        "  # Removing columns with 'object' dtype\n",
        "  df_without_objects = dataframe.drop(object_columns, axis=1)\n",
        "\n",
        "  # Creating separate variables for object type columns with their respective names\n",
        "  for col in object_columns:\n",
        "    globals()[f\"{col}\"] = dataframe[col]\n",
        "\n",
        "  # Printing separate variables\n",
        "  for col in object_columns:\n",
        "    print(f\"{col}:\")\n",
        "    print(globals()[f\"{col}\"])\n",
        "\n",
        "  # Returning the original DataFrame without object type columns\n",
        "  return df_without_objects"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZZxioIG1dVr3"
      },
      "source": [
        "- Input the name of the index variable, which will be integrated in the output dataframe.\n",
        "\n",
        "  An example would be the following:\n",
        "  > *Enter the index variable from the list of variables:* **PageName**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B2GnXxNMdeOp"
      },
      "outputs": [],
      "source": [
        "index_variable = input(\"Enter the index variable from the list of variables: \")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-lDqtDV0ZyLw"
      },
      "outputs": [],
      "source": [
        "test_data = process_object_columns(test_data, index_variable)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SF8DJq5vFHhP"
      },
      "source": [
        "- The index variable is extracted from the *test_data* variable and stored in **test_page**.\n",
        "\n",
        "- The column specified by the index variable is removed from the *test_data*.\n",
        "- The updated **test_data** is printed below.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KOke3sHySjdh"
      },
      "outputs": [],
      "source": [
        "test_page = test_data[index_variable]\n",
        "test_data.drop(columns=[index_variable], inplace=True)\n",
        "print(test_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "niecylJZc821"
      },
      "source": [
        "- The next step is to create a variable that holds date columns.\n",
        "  - Since the *date* column is a component of our predictor variables, we must format its numerical values to a value that GBoost can interpret as a date, rather than just a number. However, before formatting the *date* column, we create this **date** variable to preserve its original format and then insert it back into the output frame. The date format that can be taken can be found in the **Date Formatting** section."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HRhzSPisFQuI"
      },
      "outputs": [],
      "source": [
        "date = pd.read_csv(test_file, usecols=['Date'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3LdbwkJwdQtH"
      },
      "source": [
        "# Date Formatting\n",
        "- For regression models it is important to follow the required formatting to make sure to have a high perfomance. This section will change the date format to a *to_datetime* format which works better with machine learning regression models.Therefore, there will be a date formatting for both testing and training sets.\n",
        ">**Note:**\n",
        "> - This section only allows three types of date formats : \"%m/%d/%Y\", \"%Y/%m/%d\", and \"%Y/%j\". If you are using another type of format make sure to change it to one of those formats beforehand.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j-dnW4ChdUak"
      },
      "outputs": [],
      "source": [
        "def date_formatting(dataframe2):\n",
        "    # Processing 'Date' column for testing\n",
        "    date_format = input('If file use \"%m/%d/%Y\" type: A, if \"%Y/%m/%d\" type: B, else \"%Y/%j\" type C: ')\n",
        "    #date_column = dataframe2[column_name]\n",
        "    while date_format != 'A' and date_format != 'B' and date_format != 'C':\n",
        "        date_format = input('Input ERROR. If file use \"%m/%d/%Y\" type: A, if \"%Y/%m/%d\" type: B, else if \"%Y/%j\" type C: ')\n",
        "\n",
        "    if date_format == 'A':\n",
        "        #date_format = '%m%d%Y'\n",
        "        dataframe2['Date'] = pd.to_datetime(date['Date'], format=\"%m/%d/%Y\")\n",
        "        dataframe2['Date'] =  dataframe2['Date'].astype(int)\n",
        "\n",
        "    if date_format == 'B':\n",
        "        #date_format = '%Y/%m/%d'\n",
        "        dataframe2['Date'] = pd.to_datetime(date['Date'], format = '%Y/%m/%d')\n",
        "        #.astype(int)\n",
        "        dataframe2['Date'] =  dataframe2['Date'].astype(int)\n",
        "\n",
        "    if date_format == 'C':\n",
        "        #date_format = '%Y/%j'\n",
        "        dataframe2['Date'] = pd.to_datetime(date['Date'], format=\"%Y/%j\")\n",
        "        dataframe2['Date'] =  dataframe2['Date'].astype(int)\n",
        "    #else:\n",
        "      #print (\"Data format not recognized, make sure your data is formatted as %m%d%Y, %Y%m%d, or %Y%j\")\n",
        "     # date_format = 0\n",
        "    return dataframe2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AlMHb6PUdWsu"
      },
      "outputs": [],
      "source": [
        "test_data = date_formatting(test_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rxUJPNDudYll"
      },
      "source": [
        "*   Lastly, to get the testing data ready, the predictor variables should be separated from the target variable.\n",
        "- An example of the target variable is the following:\n",
        "> *Enter the name of the target variable to extract it from the list of variables:* **Smerge**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N5zAYGRrddyu"
      },
      "outputs": [],
      "source": [
        "# Separating target variable (dependent) from the initial variables (independent).\n",
        "target_variable = input(\"Enter the name of the target variable to extract it from the list of variables: \")\n",
        "variables = test_data.columns.tolist()\n",
        "\n",
        "while target_variable in variables:\n",
        "    variables.remove(target_variable)\n",
        "\n",
        "y_test = test_data[target_variable]\n",
        "x_test = test_data[variables]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CA1ASrYBbL29"
      },
      "source": [
        "# Training Data\n",
        "\n",
        "- First, we create a variable called *train_data* to read our training data. Then, using *variables* from the previous section, we save all of the data into *train_data*. The training data will be used to create our model and make it able to create predictions.\n",
        "\n",
        "  > **Note:**\n",
        "  > - Since we already removed the object type columns in a previous section we will use  *variables* from the **Variables** section to retrieve just the numerical columns values and define **x_train**. Addtionally, we will use **target_variable** from testing data to also define **y_train** in the training data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K97dOvQMcPGW"
      },
      "outputs": [],
      "source": [
        "train_data = pd.read_csv(train_file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nl8T8pPbdoLX"
      },
      "source": [
        "- Prior to defining the predictor variables (**x_train**) and target variable (**y_train**) for training, we must also format the date for the training data.\n",
        "  - To do this, we will use the same function that we used in the **Date Formatting** section."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fAK4Hr7YdqXH"
      },
      "outputs": [],
      "source": [
        "train_data.dropna(inplace=True)\n",
        "train_data = date_formatting(train_data)\n",
        "train_data.drop(columns=[index_variable], inplace=True)\n",
        "# variables.remove(index_variable)\n",
        "# Defining x_train and y_train\n",
        "x_train = train_data[variables]\n",
        "y_train = train_data[target_variable]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PsWTklbtSjdi"
      },
      "outputs": [],
      "source": [
        "x_train = train_data[variables]\n",
        "y_train = train_data[target_variable]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y0QsiMo7fQF7"
      },
      "source": [
        "# Gradient Boosting Regressor\n",
        "\n",
        "Gradient Boosting is an ML technique from Sklearning where it is a tree-based algorithm and works to obtain continuous predictions until it results in a more accurate overall prediction. The hypertuning parameters were conducted using an iteration method. A series of tuning values were tested and the best parameters were found after hundreds of trials.\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "- **n_estimators** - *int*; The number of times to repeat the process of training a model on the residuals of the previous model. A large number of iterations will result in better performance.\n",
        "- **max_depth** - *int* or *None*; The maximum number of levels in the decision tree.\n",
        "- **min_sample_split** - *int, float*; The minimum number of samples required to split an internal node:\n",
        "  - If *int*, values must be in the range [2, infinity).\n",
        "  - If *float*, values must be in range (0.0, 1.0] and *min_samples_split* will be *ceil(min_samples_split* * *n_samples*).\n",
        "- **learning_rate** - *float*; Controls the size of the steps taken towards the minimum of a loss function at each iteration.\n",
        "- **loss** - *{'squared_error', 'absolute_error', 'huber', 'quantile'}*; The objective function:\n",
        "  - ***squared_error*** - refers to the squared error for regression,\n",
        "  - ***absolute_error*** - refers to the absolute error of regression and is a robust loss function,\n",
        "  - ***huber*** - is a combination '*squared_error*' and '*absolute_error*',\n",
        "  - ***quantile*** - allows quantile regression.\n",
        "- **verbose** - *int*; Enable verbose output:\n",
        "  - If 1 is specified, the program will print progress and performance information periodically (less frequently for larger trees).\n",
        "  - If a value greater than 1 is specified, the program will print progress and performance  information for every tree.\n",
        "\n",
        "> For more information visit: https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "The parameters that were used for GBR are listed below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I19zxKBOg4Rm"
      },
      "outputs": [],
      "source": [
        "params = {\n",
        "    \"n_estimators\": 200,\n",
        "    \"max_depth\": 10,\n",
        "    \"min_samples_split\": 4,\n",
        "    \"learning_rate\": 0.3,\n",
        "    \"loss\": \"squared_error\",\n",
        "    \"verbose\": 1\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UtUu-mTXi8Oa"
      },
      "source": [
        "- Before jumping into the machine learning configuration, there is one last processing step we need to take. Since machine learning models work best with numbers, and the formatting of the data gathered can sometimes vary, we need to convert our *x_train*, *y_train*, and *x_test* into number types using the numpy function to ensure that our algorithm runs smoothly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m9amBJzUi8ph"
      },
      "outputs": [],
      "source": [
        "x_train_nu = x_train.to_numpy()\n",
        "y_train_nu = y_train.to_numpy()\n",
        "x_test_nu = x_test.to_numpy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nwoBw_GHiiw4"
      },
      "source": [
        "*Configuring the Gradient Boosting Regressor*\n",
        "\n",
        "---\n",
        "\n",
        " - The variable *'mse'* represents the mean squared error and is obtained by using the target variable (dependent variable) and the regression prediction of the initial variables (independent variables)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ViGs01l0i0qW"
      },
      "outputs": [],
      "source": [
        "reg = ensemble.GradientBoostingRegressor(**params)\n",
        "reg.fit(x_train, y_train.values.ravel())\n",
        "\n",
        "MSE = mean_squared_error(y_test, reg.predict(x_test))\n",
        "print(\"The mean squared error (MSE) on the set: {:.4f}\".format(MSE))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UYPJcYJNSjdk"
      },
      "outputs": [],
      "source": [
        "print(x_train[x_train.isna().any(axis=1)])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hbkcTk0FkG9S"
      },
      "source": [
        "- By obtaining the prediction of the test model, we will prepare for the output dataframe.\n",
        "\n",
        "- Then we took the date that we saved earlier with its original format and incorporated it back to the output dataframe. The same steps were made for the index variable and ML.\n",
        "\n",
        "- Finally, we saved the output data frame as *.csv* using the name you inputted at the beginning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lfS3JqZikrNO"
      },
      "outputs": [],
      "source": [
        "predictions = reg.predict(x_test_nu)\n",
        "test_data['Date'] = date\n",
        "test_data['ML'] = predictions\n",
        "test_data[index_variable] = test_page\n",
        "\n",
        "## Saving the output dataframe to a CSV file\n",
        "test_data.to_csv(name + \".csv\", index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BvWOabgpmls1"
      },
      "source": [
        "# SHapley Additive exPlanations (SHAP)\n",
        "\n",
        "SHAP states the output of the ML model by carrying out the contribution from each variable to the prediction. SHAP was used to evaluate independent variable sensitivity. SHAP uses a game theory-based methodology that calculates feature importance by comparing a model's predictions with and without a particular feature. For our case, we used a random sample of 1000 from the train data. We then apply SHAP to analyze those 1000-row samples and calculate the contribution of each feature for the 1000 predictions. SHAP assigns values according to the mean marginal contribution of each feature across all possible values. Using this method, you will be able to see a bar graph that illustrates the individual contribution of each feature. The graph is arranged from the highest contribution to the lowest contribution feature.\n",
        "\n",
        "> For more information visit: https://shap.readthedocs.io/en/latest/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1kGbapw1n71z"
      },
      "outputs": [],
      "source": [
        "# Generating variable importance plots\n",
        "my_model = reg.fit(x_train, y_train)\n",
        "X_sampled = x_train.sample(1000, random_state=10, replace=True)\n",
        "\n",
        "explainer = shap.Explainer(my_model, X_sampled)\n",
        "shap_values = explainer(X_sampled)\n",
        "number_variables = len(variables)\n",
        "shap.plots.bar(shap_values, max_display=number_variables)\n",
        "\n",
        "# Saving variable importance plot\n",
        "mean_shap = np.abs(shap_values.values).mean(axis=0)\n",
        "shap_pd = pd.DataFrame(mean_shap, index=X_sampled.columns).sort_values(by=[0], ascending=False)\n",
        "shap_pd.to_csv(name + '_SHAP.csv')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}